{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOcdvcXXrySq"
      },
      "source": [
        "#Hardware#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfMFW3Xuw2Ku",
        "outputId": "9a961480-08da-4f10-8de3-dccd625ed364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Apr 20 15:49:42 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL3s3Odfiel1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "embeddings = pd.read_csv(\"/content/section_embeddings_noillustrations_24l24h.csv\")\n",
        "\n",
        "data = pd.read_csv(\"/content/sectionsdataset_withoutillustrations - Sheet1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaH9aiq7Vzxl"
      },
      "outputs": [],
      "source": [
        "# tensorembeddings = embeddings\n",
        "# for i in (embeddings.columns):\n",
        "#   tensorembeddings[i] = embeddings[i].str.extract(r'(\\d*\\.\\d*)').astype(float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGrwUj1qYY_A",
        "outputId": "29a782ee-1665-4dcc-c369-9ab1230b85bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ],
      "source": [
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qncl98kSn-tM"
      },
      "outputs": [],
      "source": [
        "sections = data[\"Description\"].values\n",
        "sectionnames = data[\"Section\"].values\n",
        "sectionnumber = data[\"Number\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzpxOv5RjHlo",
        "outputId": "846d65d0-a189-4b19-e78f-13230b814979"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506,)"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ],
      "source": [
        "sections.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXBv55lGhR_f",
        "outputId": "87848c89-aa77-4ca5-bae5-5263c68743c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "506"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ],
      "source": [
        "len(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmp6McQzXCTN"
      },
      "outputs": [],
      "source": [
        "# Convert embeddings to numpy arrays\n",
        "sentence_vectors = embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0MnyF2zQUDX",
        "outputId": "717d5338-2830-411d-b5d1-591f17263285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Some weights of BertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.12.attention.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.output.dense.bias', 'bert.encoder.layer.12.attention.output.dense.weight', 'bert.encoder.layer.12.attention.self.key.bias', 'bert.encoder.layer.12.attention.self.key.weight', 'bert.encoder.layer.12.attention.self.query.bias', 'bert.encoder.layer.12.attention.self.query.weight', 'bert.encoder.layer.12.attention.self.value.bias', 'bert.encoder.layer.12.attention.self.value.weight', 'bert.encoder.layer.12.intermediate.dense.bias', 'bert.encoder.layer.12.intermediate.dense.weight', 'bert.encoder.layer.12.output.LayerNorm.bias', 'bert.encoder.layer.12.output.LayerNorm.weight', 'bert.encoder.layer.12.output.dense.bias', 'bert.encoder.layer.12.output.dense.weight', 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.weight', 'bert.encoder.layer.13.attention.output.dense.bias', 'bert.encoder.layer.13.attention.output.dense.weight', 'bert.encoder.layer.13.attention.self.key.bias', 'bert.encoder.layer.13.attention.self.key.weight', 'bert.encoder.layer.13.attention.self.query.bias', 'bert.encoder.layer.13.attention.self.query.weight', 'bert.encoder.layer.13.attention.self.value.bias', 'bert.encoder.layer.13.attention.self.value.weight', 'bert.encoder.layer.13.intermediate.dense.bias', 'bert.encoder.layer.13.intermediate.dense.weight', 'bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.13.output.LayerNorm.weight', 'bert.encoder.layer.13.output.dense.bias', 'bert.encoder.layer.13.output.dense.weight', 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.14.attention.output.dense.bias', 'bert.encoder.layer.14.attention.output.dense.weight', 'bert.encoder.layer.14.attention.self.key.bias', 'bert.encoder.layer.14.attention.self.key.weight', 'bert.encoder.layer.14.attention.self.query.bias', 'bert.encoder.layer.14.attention.self.query.weight', 'bert.encoder.layer.14.attention.self.value.bias', 'bert.encoder.layer.14.attention.self.value.weight', 'bert.encoder.layer.14.intermediate.dense.bias', 'bert.encoder.layer.14.intermediate.dense.weight', 'bert.encoder.layer.14.output.LayerNorm.bias', 'bert.encoder.layer.14.output.LayerNorm.weight', 'bert.encoder.layer.14.output.dense.bias', 'bert.encoder.layer.14.output.dense.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.bias', 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.dense.bias', 'bert.encoder.layer.15.attention.output.dense.weight', 'bert.encoder.layer.15.attention.self.key.bias', 'bert.encoder.layer.15.attention.self.key.weight', 'bert.encoder.layer.15.attention.self.query.bias', 'bert.encoder.layer.15.attention.self.query.weight', 'bert.encoder.layer.15.attention.self.value.bias', 'bert.encoder.layer.15.attention.self.value.weight', 'bert.encoder.layer.15.intermediate.dense.bias', 'bert.encoder.layer.15.intermediate.dense.weight', 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.15.output.LayerNorm.weight', 'bert.encoder.layer.15.output.dense.bias', 'bert.encoder.layer.15.output.dense.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.bias', 'bert.encoder.layer.16.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.dense.bias', 'bert.encoder.layer.16.attention.output.dense.weight', 'bert.encoder.layer.16.attention.self.key.bias', 'bert.encoder.layer.16.attention.self.key.weight', 'bert.encoder.layer.16.attention.self.query.bias', 'bert.encoder.layer.16.attention.self.query.weight', 'bert.encoder.layer.16.attention.self.value.bias', 'bert.encoder.layer.16.attention.self.value.weight', 'bert.encoder.layer.16.intermediate.dense.bias', 'bert.encoder.layer.16.intermediate.dense.weight', 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.16.output.LayerNorm.weight', 'bert.encoder.layer.16.output.dense.bias', 'bert.encoder.layer.16.output.dense.weight', 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.attention.output.LayerNorm.weight', 'bert.encoder.layer.17.attention.output.dense.bias', 'bert.encoder.layer.17.attention.output.dense.weight', 'bert.encoder.layer.17.attention.self.key.bias', 'bert.encoder.layer.17.attention.self.key.weight', 'bert.encoder.layer.17.attention.self.query.bias', 'bert.encoder.layer.17.attention.self.query.weight', 'bert.encoder.layer.17.attention.self.value.bias', 'bert.encoder.layer.17.attention.self.value.weight', 'bert.encoder.layer.17.intermediate.dense.bias', 'bert.encoder.layer.17.intermediate.dense.weight', 'bert.encoder.layer.17.output.LayerNorm.bias', 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.17.output.dense.bias', 'bert.encoder.layer.17.output.dense.weight', 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'bert.encoder.layer.18.attention.output.dense.bias', 'bert.encoder.layer.18.attention.output.dense.weight', 'bert.encoder.layer.18.attention.self.key.bias', 'bert.encoder.layer.18.attention.self.key.weight', 'bert.encoder.layer.18.attention.self.query.bias', 'bert.encoder.layer.18.attention.self.query.weight', 'bert.encoder.layer.18.attention.self.value.bias', 'bert.encoder.layer.18.attention.self.value.weight', 'bert.encoder.layer.18.intermediate.dense.bias', 'bert.encoder.layer.18.intermediate.dense.weight', 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.18.output.dense.bias', 'bert.encoder.layer.18.output.dense.weight', 'bert.encoder.layer.19.attention.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.output.dense.bias', 'bert.encoder.layer.19.attention.output.dense.weight', 'bert.encoder.layer.19.attention.self.key.bias', 'bert.encoder.layer.19.attention.self.key.weight', 'bert.encoder.layer.19.attention.self.query.bias', 'bert.encoder.layer.19.attention.self.query.weight', 'bert.encoder.layer.19.attention.self.value.bias', 'bert.encoder.layer.19.attention.self.value.weight', 'bert.encoder.layer.19.intermediate.dense.bias', 'bert.encoder.layer.19.intermediate.dense.weight', 'bert.encoder.layer.19.output.LayerNorm.bias', 'bert.encoder.layer.19.output.LayerNorm.weight', 'bert.encoder.layer.19.output.dense.bias', 'bert.encoder.layer.19.output.dense.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.attention.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.output.dense.bias', 'bert.encoder.layer.20.attention.output.dense.weight', 'bert.encoder.layer.20.attention.self.key.bias', 'bert.encoder.layer.20.attention.self.key.weight', 'bert.encoder.layer.20.attention.self.query.bias', 'bert.encoder.layer.20.attention.self.query.weight', 'bert.encoder.layer.20.attention.self.value.bias', 'bert.encoder.layer.20.attention.self.value.weight', 'bert.encoder.layer.20.intermediate.dense.bias', 'bert.encoder.layer.20.intermediate.dense.weight', 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.encoder.layer.20.output.dense.bias', 'bert.encoder.layer.20.output.dense.weight', 'bert.encoder.layer.21.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.weight', 'bert.encoder.layer.21.attention.output.dense.bias', 'bert.encoder.layer.21.attention.output.dense.weight', 'bert.encoder.layer.21.attention.self.key.bias', 'bert.encoder.layer.21.attention.self.key.weight', 'bert.encoder.layer.21.attention.self.query.bias', 'bert.encoder.layer.21.attention.self.query.weight', 'bert.encoder.layer.21.attention.self.value.bias', 'bert.encoder.layer.21.attention.self.value.weight', 'bert.encoder.layer.21.intermediate.dense.bias', 'bert.encoder.layer.21.intermediate.dense.weight', 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.21.output.LayerNorm.weight', 'bert.encoder.layer.21.output.dense.bias', 'bert.encoder.layer.21.output.dense.weight', 'bert.encoder.layer.22.attention.output.LayerNorm.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.output.dense.bias', 'bert.encoder.layer.22.attention.output.dense.weight', 'bert.encoder.layer.22.attention.self.key.bias', 'bert.encoder.layer.22.attention.self.key.weight', 'bert.encoder.layer.22.attention.self.query.bias', 'bert.encoder.layer.22.attention.self.query.weight', 'bert.encoder.layer.22.attention.self.value.bias', 'bert.encoder.layer.22.attention.self.value.weight', 'bert.encoder.layer.22.intermediate.dense.bias', 'bert.encoder.layer.22.intermediate.dense.weight', 'bert.encoder.layer.22.output.LayerNorm.bias', 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.22.output.dense.bias', 'bert.encoder.layer.22.output.dense.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.bias', 'bert.encoder.layer.23.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.output.dense.bias', 'bert.encoder.layer.23.attention.output.dense.weight', 'bert.encoder.layer.23.attention.self.key.bias', 'bert.encoder.layer.23.attention.self.key.weight', 'bert.encoder.layer.23.attention.self.query.bias', 'bert.encoder.layer.23.attention.self.query.weight', 'bert.encoder.layer.23.attention.self.value.bias', 'bert.encoder.layer.23.attention.self.value.weight', 'bert.encoder.layer.23.intermediate.dense.bias', 'bert.encoder.layer.23.intermediate.dense.weight', 'bert.encoder.layer.23.output.LayerNorm.bias', 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.23.output.dense.bias', 'bert.encoder.layer.23.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "# model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased',num_hidden_layers=24 , num_attention_heads=24 )\n",
        "\n",
        "# Load English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to tokenize, remove stop words, and encode sentences\n",
        "def encode_sentences(sentences):\n",
        "    cleaned_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Remove stop words\n",
        "        cleaned_sentence = ' '.join([word for word in sentence.split() if word.lower() not in stop_words])\n",
        "        cleaned_sentences.append(cleaned_sentence)\n",
        "\n",
        "    # Tokenize and encode the cleaned sentences\n",
        "    encoded_sentences = tokenizer(cleaned_sentences, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    return encoded_sentences\n",
        "\n",
        "# Encode the sections\n",
        "encoded_sections = encode_sentences(sections)\n",
        "\n",
        "# Get BERT embeddings for the tokenized sections\n",
        "def calculate_embeddings(encoded_sentences):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded_sentences)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling of token embeddings\n",
        "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=-1)  # L2 normalize\n",
        "    return embeddings\n",
        "\n",
        "#batch processing\n",
        "def batch_process_sentences(sentences, batch_size=50):\n",
        "    embeddings_list = []\n",
        "    num_batches = (len(sentences) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(num_batches):\n",
        "\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, len(sentences))\n",
        "        batch_sentences = sentences[start_idx:end_idx]\n",
        "        print(\"Encoding sections from \",start_idx,\" to \",end_idx)\n",
        "        # Encode batch of sentences\n",
        "        encoded_sentences = encode_sentences(batch_sentences)\n",
        "\n",
        "        # Calculate embeddings for batch\n",
        "        embeddings = calculate_embeddings(encoded_sentences)\n",
        "        embeddings_list.extend(embeddings)\n",
        "\n",
        "    # Concatenate embeddings from all batches\n",
        "\n",
        "    return embeddings_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-sGIzxuYlQG"
      },
      "source": [
        "#Finding similarity#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDrxf1BYWVpC"
      },
      "outputs": [],
      "source": [
        "def calculate_similarity(input_text,sentence_vectors=sentence_vectors):\n",
        "    # Encode the input text\n",
        "    encoded_input = encode_sentences([input_text])\n",
        "\n",
        "    # Get BERT embeddings for the tokenized input text\n",
        "    with torch.no_grad():\n",
        "        input_embedding = model(**encoded_input).last_hidden_state.mean(dim=1)  # Mean pooling of token embeddings\n",
        "        input_embedding = torch.nn.functional.normalize(input_embedding, p=2, dim=-1)  # L2 normalize\n",
        "\n",
        "    similarity_scores = cosine_similarity(input_embedding, sentence_vectors)\n",
        "\n",
        "    # Rank sentences based on similarity scores\n",
        "    ranked_sentences = sorted(zip(sectionnames, similarity_scores[0]), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return ranked_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QavDYa_oXdhp",
        "outputId": "5b1811ba-19c1-4ad9-e533-d811f480d393"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Coin defined ', 0.08304483199673941),\n",
              " ('Waging, or attempting to wage war, or abetting waging of war, against the Government of India',\n",
              "  0.058092611180885746),\n",
              " ('Counterfeiting Government stamp ', 0.051316379677564236),\n",
              " ('Fraudulently or dishonestly diminishing weight or altering composition of coin ',\n",
              "  0.05005387767714778),\n",
              " ('Person employed in mint causing coin to be of different weight or composition from that fixed by law ',\n",
              "  0.04954828246758841),\n",
              " ('Counterfeiting device or mark used for authenticating documents other than those described in section 467, or possessing counterfeit marked material ',\n",
              "  0.049271018106036964),\n",
              " ('Making or selling instrument for counterfeiting Government stamp ',\n",
              "  0.04859598548106513),\n",
              " ('Forged document ', 0.04835798420392587),\n",
              " ('Possession of Indian coin by person who knew it to be counterfeit when he became possessed thereof ',\n",
              "  0.04685408409780542),\n",
              " ('Fraudulently or dishonestly diminishing weight or altering composition of Indian coin ',\n",
              "  0.045913036516037065)]"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ],
      "source": [
        "calculate_similarity(\"A government official misuses his position and stole public funds\")[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH-McmIvd2QL",
        "outputId": "84b2d80b-dcc6-4d68-8cdf-9e90aafbd5e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "506"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ],
      "source": [
        "len(sections)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRQ4JH3UjSKe",
        "outputId": "a86adc00-404a-40d1-d956-01c029ced239"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Whoever gives to any public servant any information which he knows or believes to be false, intending thereby to cause, or knowing it to be likely that he will thereby cause such public servant -to do or omit anything which such public servant ought not to do or omit if the true state of facts respecting which such information is given were known by him, orto use the lawful power of such public servant to the injury or annoyance of any person, shall be punished with imprisonment of either description for a term which may extend to six months, or with fine which may extend to one thousand rupees, or with both.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ],
      "source": [
        "sections[sectionnames=='False information, with intent to cause public servant to use his lawful power to the injury of another person']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "aIV-UmYAlvrO",
        "outputId": "e0db4f8b-a98e-4f8a-e1e3-0650ab931c82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "0    0.006125 -0.032665  0.014649  0.026808  0.059089 -0.009365 -0.016808   \n",
              "1    0.026880 -0.030728  0.010022  0.008024  0.059553 -0.041781 -0.033307   \n",
              "2   -0.014643 -0.044208  0.016928  0.013390  0.019684 -0.030419 -0.008244   \n",
              "3   -0.009372 -0.051847  0.017720 -0.008822  0.035534 -0.017540 -0.023276   \n",
              "4   -0.003544 -0.027803  0.003720 -0.005375  0.063969 -0.028282 -0.026335   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "501 -0.007725 -0.035536  0.022585 -0.008584  0.045320 -0.019671 -0.028757   \n",
              "502 -0.019404 -0.029230  0.012766 -0.014646  0.037214 -0.036061 -0.017597   \n",
              "503 -0.014019  0.002406  0.032784 -0.012924  0.054703  0.006240 -0.042927   \n",
              "504 -0.020260 -0.019837  0.027415  0.002004  0.053177 -0.026605 -0.040737   \n",
              "505 -0.005116 -0.033991  0.001709 -0.033762  0.037153 -0.029786 -0.022426   \n",
              "\n",
              "            7         8         9  ...       758       759       760  \\\n",
              "0   -0.010852 -0.042066  0.013446  ... -0.002032 -0.064723 -0.066550   \n",
              "1    0.024806 -0.025195  0.005954  ... -0.004732 -0.088148 -0.040419   \n",
              "2    0.029799 -0.006144  0.012375  ... -0.005040 -0.091151 -0.047512   \n",
              "3    0.029102 -0.031280  0.014001  ...  0.018803 -0.095583 -0.047304   \n",
              "4   -0.014699 -0.030639  0.020134  ...  0.000567 -0.089807 -0.029630   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "501  0.020993 -0.016390  0.015359  ...  0.005521 -0.070507 -0.047128   \n",
              "502  0.015215 -0.017839  0.014814  ...  0.008720 -0.097858 -0.038218   \n",
              "503  0.008384 -0.005286  0.028453  ... -0.018657 -0.054719 -0.030794   \n",
              "504  0.008822 -0.004693  0.043284  ...  0.002584 -0.057660 -0.039622   \n",
              "505 -0.005727 -0.013404  0.009893  ... -0.012713 -0.074701 -0.044560   \n",
              "\n",
              "          761       762       763       764       765       766       767  \n",
              "0   -0.009346  0.004759  0.030237  0.056819  0.062340  0.046500  0.017790  \n",
              "1   -0.013646  0.006121  0.009879  0.072345  0.033862  0.025697  0.026197  \n",
              "2   -0.023470  0.003134  0.009436  0.067596  0.018500  0.028217 -0.014844  \n",
              "3   -0.004684  0.016457  0.024335  0.071600  0.017466  0.041474  0.006945  \n",
              "4   -0.027142  0.003355  0.019631  0.069448  0.022996  0.030006  0.017287  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "501 -0.011143  0.016921 -0.007654  0.064237  0.016454  0.037350  0.027830  \n",
              "502 -0.008666  0.016269 -0.026597  0.055956  0.004337  0.053822  0.021270  \n",
              "503 -0.010900  0.022542 -0.024449  0.048990  0.019509  0.029780  0.017413  \n",
              "504 -0.013349  0.017212 -0.038237  0.041818  0.022263  0.026249  0.025970  \n",
              "505 -0.003995  0.004702  0.015139  0.075416  0.033654  0.052404  0.021356  \n",
              "\n",
              "[506 rows x 768 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-08d6c0d0-57fa-4d85-a2d1-4c9b96020c7a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.006125</td>\n",
              "      <td>-0.032665</td>\n",
              "      <td>0.014649</td>\n",
              "      <td>0.026808</td>\n",
              "      <td>0.059089</td>\n",
              "      <td>-0.009365</td>\n",
              "      <td>-0.016808</td>\n",
              "      <td>-0.010852</td>\n",
              "      <td>-0.042066</td>\n",
              "      <td>0.013446</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002032</td>\n",
              "      <td>-0.064723</td>\n",
              "      <td>-0.066550</td>\n",
              "      <td>-0.009346</td>\n",
              "      <td>0.004759</td>\n",
              "      <td>0.030237</td>\n",
              "      <td>0.056819</td>\n",
              "      <td>0.062340</td>\n",
              "      <td>0.046500</td>\n",
              "      <td>0.017790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.026880</td>\n",
              "      <td>-0.030728</td>\n",
              "      <td>0.010022</td>\n",
              "      <td>0.008024</td>\n",
              "      <td>0.059553</td>\n",
              "      <td>-0.041781</td>\n",
              "      <td>-0.033307</td>\n",
              "      <td>0.024806</td>\n",
              "      <td>-0.025195</td>\n",
              "      <td>0.005954</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004732</td>\n",
              "      <td>-0.088148</td>\n",
              "      <td>-0.040419</td>\n",
              "      <td>-0.013646</td>\n",
              "      <td>0.006121</td>\n",
              "      <td>0.009879</td>\n",
              "      <td>0.072345</td>\n",
              "      <td>0.033862</td>\n",
              "      <td>0.025697</td>\n",
              "      <td>0.026197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.014643</td>\n",
              "      <td>-0.044208</td>\n",
              "      <td>0.016928</td>\n",
              "      <td>0.013390</td>\n",
              "      <td>0.019684</td>\n",
              "      <td>-0.030419</td>\n",
              "      <td>-0.008244</td>\n",
              "      <td>0.029799</td>\n",
              "      <td>-0.006144</td>\n",
              "      <td>0.012375</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.005040</td>\n",
              "      <td>-0.091151</td>\n",
              "      <td>-0.047512</td>\n",
              "      <td>-0.023470</td>\n",
              "      <td>0.003134</td>\n",
              "      <td>0.009436</td>\n",
              "      <td>0.067596</td>\n",
              "      <td>0.018500</td>\n",
              "      <td>0.028217</td>\n",
              "      <td>-0.014844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.009372</td>\n",
              "      <td>-0.051847</td>\n",
              "      <td>0.017720</td>\n",
              "      <td>-0.008822</td>\n",
              "      <td>0.035534</td>\n",
              "      <td>-0.017540</td>\n",
              "      <td>-0.023276</td>\n",
              "      <td>0.029102</td>\n",
              "      <td>-0.031280</td>\n",
              "      <td>0.014001</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018803</td>\n",
              "      <td>-0.095583</td>\n",
              "      <td>-0.047304</td>\n",
              "      <td>-0.004684</td>\n",
              "      <td>0.016457</td>\n",
              "      <td>0.024335</td>\n",
              "      <td>0.071600</td>\n",
              "      <td>0.017466</td>\n",
              "      <td>0.041474</td>\n",
              "      <td>0.006945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.003544</td>\n",
              "      <td>-0.027803</td>\n",
              "      <td>0.003720</td>\n",
              "      <td>-0.005375</td>\n",
              "      <td>0.063969</td>\n",
              "      <td>-0.028282</td>\n",
              "      <td>-0.026335</td>\n",
              "      <td>-0.014699</td>\n",
              "      <td>-0.030639</td>\n",
              "      <td>0.020134</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000567</td>\n",
              "      <td>-0.089807</td>\n",
              "      <td>-0.029630</td>\n",
              "      <td>-0.027142</td>\n",
              "      <td>0.003355</td>\n",
              "      <td>0.019631</td>\n",
              "      <td>0.069448</td>\n",
              "      <td>0.022996</td>\n",
              "      <td>0.030006</td>\n",
              "      <td>0.017287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>-0.007725</td>\n",
              "      <td>-0.035536</td>\n",
              "      <td>0.022585</td>\n",
              "      <td>-0.008584</td>\n",
              "      <td>0.045320</td>\n",
              "      <td>-0.019671</td>\n",
              "      <td>-0.028757</td>\n",
              "      <td>0.020993</td>\n",
              "      <td>-0.016390</td>\n",
              "      <td>0.015359</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005521</td>\n",
              "      <td>-0.070507</td>\n",
              "      <td>-0.047128</td>\n",
              "      <td>-0.011143</td>\n",
              "      <td>0.016921</td>\n",
              "      <td>-0.007654</td>\n",
              "      <td>0.064237</td>\n",
              "      <td>0.016454</td>\n",
              "      <td>0.037350</td>\n",
              "      <td>0.027830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>-0.019404</td>\n",
              "      <td>-0.029230</td>\n",
              "      <td>0.012766</td>\n",
              "      <td>-0.014646</td>\n",
              "      <td>0.037214</td>\n",
              "      <td>-0.036061</td>\n",
              "      <td>-0.017597</td>\n",
              "      <td>0.015215</td>\n",
              "      <td>-0.017839</td>\n",
              "      <td>0.014814</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008720</td>\n",
              "      <td>-0.097858</td>\n",
              "      <td>-0.038218</td>\n",
              "      <td>-0.008666</td>\n",
              "      <td>0.016269</td>\n",
              "      <td>-0.026597</td>\n",
              "      <td>0.055956</td>\n",
              "      <td>0.004337</td>\n",
              "      <td>0.053822</td>\n",
              "      <td>0.021270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>-0.014019</td>\n",
              "      <td>0.002406</td>\n",
              "      <td>0.032784</td>\n",
              "      <td>-0.012924</td>\n",
              "      <td>0.054703</td>\n",
              "      <td>0.006240</td>\n",
              "      <td>-0.042927</td>\n",
              "      <td>0.008384</td>\n",
              "      <td>-0.005286</td>\n",
              "      <td>0.028453</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018657</td>\n",
              "      <td>-0.054719</td>\n",
              "      <td>-0.030794</td>\n",
              "      <td>-0.010900</td>\n",
              "      <td>0.022542</td>\n",
              "      <td>-0.024449</td>\n",
              "      <td>0.048990</td>\n",
              "      <td>0.019509</td>\n",
              "      <td>0.029780</td>\n",
              "      <td>0.017413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>-0.020260</td>\n",
              "      <td>-0.019837</td>\n",
              "      <td>0.027415</td>\n",
              "      <td>0.002004</td>\n",
              "      <td>0.053177</td>\n",
              "      <td>-0.026605</td>\n",
              "      <td>-0.040737</td>\n",
              "      <td>0.008822</td>\n",
              "      <td>-0.004693</td>\n",
              "      <td>0.043284</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002584</td>\n",
              "      <td>-0.057660</td>\n",
              "      <td>-0.039622</td>\n",
              "      <td>-0.013349</td>\n",
              "      <td>0.017212</td>\n",
              "      <td>-0.038237</td>\n",
              "      <td>0.041818</td>\n",
              "      <td>0.022263</td>\n",
              "      <td>0.026249</td>\n",
              "      <td>0.025970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>-0.005116</td>\n",
              "      <td>-0.033991</td>\n",
              "      <td>0.001709</td>\n",
              "      <td>-0.033762</td>\n",
              "      <td>0.037153</td>\n",
              "      <td>-0.029786</td>\n",
              "      <td>-0.022426</td>\n",
              "      <td>-0.005727</td>\n",
              "      <td>-0.013404</td>\n",
              "      <td>0.009893</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.012713</td>\n",
              "      <td>-0.074701</td>\n",
              "      <td>-0.044560</td>\n",
              "      <td>-0.003995</td>\n",
              "      <td>0.004702</td>\n",
              "      <td>0.015139</td>\n",
              "      <td>0.075416</td>\n",
              "      <td>0.033654</td>\n",
              "      <td>0.052404</td>\n",
              "      <td>0.021356</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>506 rows Ã— 768 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-08d6c0d0-57fa-4d85-a2d1-4c9b96020c7a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-08d6c0d0-57fa-4d85-a2d1-4c9b96020c7a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-08d6c0d0-57fa-4d85-a2d1-4c9b96020c7a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a765de30-08bf-49df-a0c2-0ea110fe4f9b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a765de30-08bf-49df-a0c2-0ea110fe4f9b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a765de30-08bf-49df-a0c2-0ea110fe4f9b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "embed"
            }
          },
          "metadata": {},
          "execution_count": 213
        }
      ],
      "source": [
        "embed = pd.DataFrame(sentence_vectors)\n",
        "embed"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}